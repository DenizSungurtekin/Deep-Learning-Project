{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DeepLearningP1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ZIg6qLQCkQ5"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch import optim\n",
        "import dlc_pratical_prologue as prologue"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6S7Ddk31C5PE"
      },
      "source": [
        "## Data set\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSbCT8p9C_jP"
      },
      "source": [
        "N = 10\n",
        "train_input, train_target, train_classes, test_input, test_target, test_classes = prologue.generate_pair_sets(N)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "piCYIcm5JmCa",
        "outputId": "66d34b47-8c8a-4063-e99e-52c11dfb5456"
      },
      "source": [
        "# Double checking\n",
        "print(\"Training Set\")\n",
        "print(train_input.size())\n",
        "print(train_target.size())\n",
        "print(train_target.size())\n",
        "print(\"-------------------\")\n",
        "print(\"Testing Set\")\n",
        "print(test_input.size())\n",
        "print(test_target.size())\n",
        "print(test_classes.size())"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Set\n",
            "torch.Size([10, 2, 14, 14])\n",
            "torch.Size([10])\n",
            "torch.Size([10])\n",
            "-------------------\n",
            "Testing Set\n",
            "torch.Size([10, 2, 14, 14])\n",
            "torch.Size([10])\n",
            "torch.Size([10, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iei7sYugDAC1"
      },
      "source": [
        "## Helping functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-aJqi4xbDnaH"
      },
      "source": [
        "\"\"\"\n",
        "Input :-\n",
        "\n",
        "model : Pytorch NN model\n",
        "input_data : Tensor of N X 2 X 14 X 14\n",
        "target_data : Tensor of N X 1\n",
        "batch_size : Size of the batch\n",
        "\n",
        "-----------------------------------\n",
        "\n",
        "Output :-\n",
        "\n",
        "The number of samples not well classified\n",
        "\"\"\"\n",
        "def compute_erros(model,\n",
        "                  input_data,target_data,\n",
        "                  batch_size = 100):\n",
        "  nb_errors = 0\n",
        "  for inputs,targets in zip(input_data.split(batch_size),target_data.split(batch_size)):\n",
        "    outputs = model(inputs)\n",
        "    outputs = output.narrow(dim=1,start=0,length=1)\n",
        "    outputs = torch.ge(output,0.5).float()\n",
        "    for k in range(len(targets)):\n",
        "            if outputs[k] != targets[k]:\n",
        "                nb_errors += 1\n",
        "    return nb_errors\n",
        "\n",
        "\"\"\"\n",
        "Input :-\n",
        "\n",
        "model : Pytorch NN model\n",
        "train_input : Tensor of N x 2 x 14 x 14\n",
        "train_target : Tensor of N x 1\n",
        "train_classes : Tensor of N x 2\n",
        "\n",
        "test_input : Tensor of N x 2 x 14 x 14\n",
        "test_target : Tensor of N x 1\n",
        "test_classes : Tensor of N x 2\n",
        "\n",
        "epochs : the number of passes of the entire training dataset\n",
        "eta : learning parameter\n",
        "\n",
        "batch_size : Size of the batch\n",
        "\n",
        "-----------------------------------\n",
        "\n",
        "Output :-\n",
        "\n",
        "The number of samples not well classified\n",
        "\n",
        "\"\"\"\n",
        "def training_model(model,\n",
        "                   train_input,train_target,train_classes,\n",
        "                   test_input,test_target,test_classes,\n",
        "                   eta,epochs = 25,\n",
        "                   batch_size = 50):\n",
        "  \n",
        "  train_acc = []\n",
        "  test_acc = []\n",
        "\n",
        "  # define criterion and optimizer\n",
        "  # need to check the other possibilities\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = optim.SGD(model.parameters(), lr = eta)\n",
        "\n",
        "  for e in range(0,epochs):\n",
        "    for inputs,target in zip(train_input.split(batch_size),test_target.split(batch_size)):\n",
        "\n",
        "      outputs = model(inputs)\n",
        "      #checking outputs shape\n",
        "      loss = criterion(outputs,target)\n",
        "      acc_loss = acc_loss + loss.item()\n",
        "\n",
        "      # optimising parameters\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step() \n",
        "\n",
        "    train_errors = compute_nb_errors(model, train_input, train_target)\n",
        "    print(f\"Epoch # {e+1} / Train errors: {train_errors:.2f}\")\n",
        "     \n",
        "\n",
        "\n",
        "  return True\n",
        "  \n",
        "\n"
      ],
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtWUlBicKfIO"
      },
      "source": [
        "## Models"
      ]
    }
  ]
}